\documentclass{assignment}
\begin{document}

\title{5370 Midterm}


\textbf{2.} Random variable are defined by the conditional probabilities
\begin{itemize}
\item  $\Pr[Y=1 | X=0] =q$
\item  $\Pr[Y=0 | X=1]=p$
\end{itemize}
where both $p$ and $q$ are fixed probabilities.
Assume $\Pr [X=0] =\pi $ where $\pi$ is some probability.
\begin{enumerate}
\item Calculate the mutual information between $X$ and $Y$.\\
  \textbf{Solutions:}\\

  Assuming both $Y$ and $X$ are binary
  \begin{align*}
    \Pr[Y=0 | X=0] & = 1 - q \\
    \Pr[Y=1 | X=1] & = 1 - p \\
    \Pr[Y=0] & = \Pr[Y=0, X=0] + \Pr[Y=0, X=1] \\
                   & = \Pr[Y=0 | X=0]\Pr[X=0] + \Pr[Y=0 | X=1]\Pr[X=1] \\
                   & = (1- q) \pi + p(1 - \pi) \\
                   & = p + \pi (1 - q - p) \\
    \Pr[Y=1] & = \Pr[Y=1, X=0] + \Pr[Y=1, X=1] \\
                   & = \Pr[Y=1 | X=0]\Pr[X=0] + \Pr[Y=1 | X=1]\Pr[X=1] \\
                   & = q \pi + (1- p) (1 - \pi) \\
                   & = q \pi + 1 - \pi - p + p \pi \\
                   & = 1 - p + \pi (q + p - 1)
  \end{align*}
  \begin{align*}
    I(X;Y) & = H(Y) - H(Y|X) \\
    H(Y) & = -\Pr[Y=0] \log \Pr[Y=0] - \Pr[Y=1] \log \Pr[Y=1] \\
         & = - (p + \pi (1 - q - p)) \log (p + \pi (1 - q - p)) \\
         & \quad - (1 - p + \pi (q + p - 1)) \log (1 - p + \pi (q + p - 1)) \\
    H(Y|X) & = \sum_{x,y}\Pr[X=x, Y=y]\log \Pr[Y=y|X=x] \\
           & = \sum_{x, y} \Pr[X=x] \Pr[Y=y|X=x] \log \Pr[Y=y|X=x] \\
           & = \Pr[X=0]\Pr[Y=0|X=0] \log \Pr[Y=0|X=0] \\
           & \quad + \Pr[X=0]\Pr[Y=1|X=0] \log \Pr[Y=1|X=0] \\
           & \quad + \Pr[X=1]\Pr[Y=0|X=1] \log \Pr[Y=0|X=1] \\
           & \quad + \Pr[X=1]\Pr[Y=1|X=1] \log \Pr[Y=1|X=1] \\
           & = \pi (1 - q) \log (1 - q)  + \pi q \log q \\
           & \quad + (1 - \pi) p \log p + (1 - \pi) (1 - p) \log (1 - p) \\
    I(X;Y) & = - (p + \pi (1 - q - p)) \log (p + \pi (1 - q - p)) \\
           & \quad - (1 - p + \pi (q + p - 1)) \log (1 - p + \pi (q + p - 1)) \\
           & \quad - \pi (1 - q) \log (1 - q)  - \pi q \log q \\
           & \quad - (1 - \pi) p \log p - (1 - \pi) (1 - p) \log (1 - p)
  \end{align*}
\item What value of $\pi$ maximizes the mutual information? \\
  \textbf{Solution:} \\
  \begin{align*}
    \df{d}{d\pi} I(X;Y)
    & = -(1-q-p)\log (p + \pi (1 - q - p)) - 1 + q + p \\
    & \quad - (q + p - 1)\log (1 - p + \pi (q + p - 1)) - q - p + 1 \\
    & \quad - (1 - q) \log (1- q) - q \log q+ p \log p + (1 - p) \log (1 - p) \\
    & = -(1-q-p)\log (p + \pi (1 - q - p)) \\
    & \quad - (q + p - 1)\log (1 - p + \pi (q + p - 1)) \\
    & \quad - (1 - q) \log (1- q) - q \log q+ p \log p + (1 - p) \log (1 - p)
  \end{align*}
\end{enumerate}

\end{document}

